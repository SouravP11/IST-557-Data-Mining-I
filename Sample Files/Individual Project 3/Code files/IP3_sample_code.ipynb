{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load and check data","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/tweet-data/train.csv\")\ntest_x = pd.read_csv(\"/kaggle/input/tweet-data/test_x.csv\")\ntrain","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocess text\nYou may consider applying some preprocessing steps first, such as:\n- URL data removal;\n- Turn to lowercase;\n- Remove hashtags start with @;\n- Remove special characters (e.g., =!#%@$^&*)\n- Lemmatization (convert words to base form): https://www.nltk.org/howto/stem.html \n- ...","metadata":{}},{"cell_type":"code","source":"import re\n\n# Example: URL removal\ndef remove_url(data):\n    re1 = r\"http://\\S+|www\\.\\S+\"\n    re2 = r\"https://\\S+|www\\.\\S+\"\n    url_clean= re.compile(\"(%s|%s)\" % (re1, re2))\n    data=url_clean.sub(r'',data)\n    return data\n\n# !!! YOUR TASK 1 (4 point): add TWO more preprocessing steps from the above list\n# you can choose two functions to implement\ndef to_lowercase(data):\n    # your implementation\n    \n    return data\n\ndef remove_hashtag(data):\n    # your implementation\n    \n    return data\n\ndef remove_special(data):\n    # your implementation\n    \n    return data\n\ndef lemmatize(data):\n    # your implementation\n    \n    return data\n\n\nprint('before removal: ', train.iloc[5325]['text'])\n\ntrain['text'] = train['text'].apply(remove_url)\ntest_x['text'] = test_x['text'].apply(remove_url)\n\nprint('after removal: ', train.iloc[5325]['text'])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Classification based on bag-of-word text features\n\n\nGenerating hand-crafted bag-of-word feature vectors:\n- Assign a fixed index to each word occurring in any document of the training set (for instance by building a dictionary from words to integer indices). Each word essentially is an one-hot embedding (dimension=vocab size);\n- For each document #i, count the number of occurrences of each word w and store it in X[i, j] as the value of feature #j where j is the index of word w in the dictionary. Each document is a summation of one-hot embedding of all words.\n- You could filter out words that are rarely or too frequently appeared in documents.\n\nsklearn tutorial: https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html","metadata":{}},{"cell_type":"code","source":"# This package generates bag of words feature for each text\nfrom sklearn.feature_extraction.text import CountVectorizer \n\n# doc for this function: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\nvectorizer = CountVectorizer(stop_words='english', max_df=1.0, min_df=1, max_features=10000) \n\n# use training data to construct vocabulary and turn training text into BOW vectors\ntrain_bow = vectorizer.fit_transform(train['text'])\nvocab = vectorizer.vocabulary_.copy()\n# You can change the vocab size by tuning \"max_df\", \"min_df\" or \"max_features\" in \"CountVectorizer\"\nprint('vocab size: ', len(vocab))  \nprint (train_bow.shape) # num_train_text * vocab_size\n\n# !!! YOUR TASK 2 (1 point): transform test text into BOW vectors using vectorizer\ntest_bow = \nprint (test_bow.shape) # num_train_text * vocab_size","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Use basic ML models on BOW features\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import recall_score\nfrom sklearn.model_selection import train_test_split\n\n# Split given training data into training and validation set\nX_train,X_val,y_train,y_val = train_test_split(train_bow, train['target'], test_size=0.3, random_state=11)\n\nLR = RandomForestClassifier()\nLR.fit(X_train.toarray(),y_train)\nprint('training recall: ', recall_score(y_train, LR.predict(X_train.toarray())))\nprint('validation recall: ', recall_score(y_val, LR.predict(X_val.toarray())))\n\n# Optional: apply more advanced model selection technique\n# e.g., sklearn.model_selection.GridSearchCV: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Classification based on word2vec embedding features\n\nword2vec:\n- Skip-grams or CBOW\n- low-dimensional feature vectors with semantic information preserved\n \nBefore deep learning pervades, gensim is a popular text mining package. Here is a tutorial of gensim's word2vec: https://radimrehurek.com/gensim/models/word2vec.html\n\nNow with deep learning, pytorch and tensorflow all has their way to realize word2vec. Tutorial: https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html","metadata":{}},{"cell_type":"code","source":"# train word embeddings using gensim library\n\nfrom gensim.models import Word2Vec\n\ntrain_docs = [train['text'].iloc[i].split(' ') for i in range(len(train['text']))] # gensim only takes list of word list as `sentences`\nw2v_model = Word2Vec(sentences=train_docs, vector_size=100, window=5, negative=20, min_count=2, workers=1)\nw2v_model.train(train_docs, total_examples=len(train_docs), epochs=10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check the learned word embeddings\n\n# vocabulary\nw2v_vocav = w2v_model.wv.index_to_key\nprint('example words in vocab: ', [w2v_vocav[i] for i in range(200)])\n\n# example embedding\nexample_word = 'bomb'\nword_vec = w2v_model.wv[example_word]\nprint('learned embedding of bomb: ', word_vec)\n\n# !!! YOUR TASK (2 point): show the top-10 words most similar to 'bomb'\n# you may calculate manually, or refer to gensim function: https://radimrehurek.com/gensim/models/word2vec.html\n\n\n\n# !!! Your TASK (1 point): tune the arguments of Word2Vec to obtain different word embeddings \n# and show top-10 similar words as ''\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Use basic ML models on word embeddings\n\n# !!! YOUR TASK (2 points): turn word embedding to doc embedding\n# Choice 1 (as shown below): manualy take the average of word embeddings in each doc\n# Choise 2: gensim doc2vec: https://radimrehurek.com/gensim/models/doc2vec.html \ndef doc2vec(docs, w2v_model):\n    w2v = []\n    for doc in docs:\n        word_embs = [w2v_model.wv[w] for w in doc if w in w2v_model.wv] # get a list of word embeddings\n        \n        # TODO: take (weighted) average to get doc embedding\n        doc_emb = np.mean(np.array(word_embs), axis=0) \n        \n        # filling zeros for empty doc\n        if len(word_embs) == 0:\n            doc_emb = [0 for i in range(w2v_model.vector_size)]\n        \n        w2v.append(doc_emb)\n    \n    return np.array(w2v)\n        \n# transform training tweets into embeddings\ntrain_w2v = doc2vec(train_docs, w2v_model)\nprint('training word2vec shape: ', train_w2v.shape)\n\ntest_w2v = doc2vec(test_x['text'], w2v_model) \nprint('test word2vec shape: ', test_w2v.shape)\n\n\n# !!! YOUR TASK (2 point): apply a basic ML model on word embeddings\n# you may consider tune the word2vec model to gain better embeddings (refer to gensim document about argument details: https://radimrehurek.com/gensim/models/word2vec.html)\nX_train,X_val,y_train,y_val = train_test_split(train_w2v, train['target'], test_size=0.3, random_state=11)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Optional (+1 bonus pt): Classification directly using RNN model\nRNN model can take in word, turn them into embedding and make predictions\n\nTorch tutorial: https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html","metadata":{}},{"cell_type":"code","source":"# Preprocess text via tokenizer\n\nfrom torchtext.data.utils import get_tokenizer\nfrom torchtext.vocab import build_vocab_from_iterator\n\ntokenizer = get_tokenizer(\"basic_english\")\ntrain_iter = train['text']\n\n\ndef yield_tokens(data_iter):\n    for text in data_iter:\n        yield tokenizer(text)\n\n\nvocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\nvocab.set_default_index(vocab[\"<unk>\"])\nprint('size of vocab: ', len(vocab))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Build a simple RNN model\n\nfrom torch import nn\n\n\n# RNN can be a combination of feature extractor (word2vec) and a classifer\nclass TextClassificationModel(nn.Module):\n    def __init__(self, vocab_size, embed_dim, num_class):\n        super(TextClassificationModel, self).__init__()\n        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=False) # word2vec\n        self.fc = nn.Linear(embed_dim, num_class) # classifier\n        self.init_weights()\n\n    def init_weights(self):\n        initrange = 0.5\n        self.embedding.weight.data.uniform_(-initrange, initrange)\n        self.fc.weight.data.uniform_(-initrange, initrange)\n        self.fc.bias.data.zero_()\n\n    def forward(self, text, offsets):\n        embedded = self.embedding(text, offsets)\n        return self.fc(embedded)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Optional Task (+1 bonus point): follow the tutorial to train RNN on the given dataset\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Save submission file","metadata":{}},{"cell_type":"code","source":"y_pred = pd.DataFrame({\n    'id': test_x['id'],\n    'target': !!! # your prediction\n    }\n)\n\ny_pred.to_csv(\"test_submission.csv\", index=False)\ny_pred.head(3)","metadata":{},"execution_count":null,"outputs":[]}]}