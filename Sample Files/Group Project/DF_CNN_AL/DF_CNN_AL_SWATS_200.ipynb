{"cells":[{"cell_type":"markdown","metadata":{},"source":["Import Statements"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision\n","from torchvision import datasets, models, transforms\n","from torch.utils.data import DataLoader, random_split\n","import numpy as np\n","import random\n","from sklearn.metrics import f1_score, recall_score\n","import matplotlib.pyplot as plt\n","import torch.nn.functional as F"]},{"cell_type":"markdown","metadata":{},"source":["Device Configuration"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["device: cuda:0\n"]}],"source":["# Set device\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"device: {device}\")"]},{"cell_type":"markdown","metadata":{},"source":["Data Prep"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Files already downloaded and verified\n","Files already downloaded and verified\n"]}],"source":["# Download and load the CIFAR-10 dataset\n","transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n","train_set = datasets.CIFAR10(root='../data', train=True, download=True, transform=transform)\n","test_set = datasets.CIFAR10(root='../data', train=False, download=True, transform=transform)\n","\n","# Split train data into train and validation\n","train_size = int(0.8 * len(train_set))\n","val_size = len(train_set) - train_size\n","train_dataset, val_dataset = random_split(train_set, [train_size, val_size])\n","\n","batch_size = 1024\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n","test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=2)"]},{"cell_type":"markdown","metadata":{},"source":["Random Seed"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["# Function to set a random seed for reproducibility\n","def set_seed(seed=None):\n","    if seed is None:\n","        seed = np.random.randint(0, 10000)\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    return seed"]},{"cell_type":"markdown","metadata":{},"source":["Validation"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["# Function for validation\n","def validate(net, val_loader, criterion):\n","    total_loss = 0.0\n","    with torch.no_grad():\n","        for data in val_loader:\n","            images, labels = data[0].to(device), data[1].to(device)\n","            outputs = net(images)\n","            loss = criterion(outputs, labels)\n","            total_loss += loss.item()\n","    return total_loss / len(val_loader)"]},{"cell_type":"markdown","metadata":{},"source":["Train Model"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["# Function for training\n","def train_model(net, train_loader, val_loader, criterion, optimizer, epochs):\n","    train_losses = []\n","    val_losses = []\n","    for epoch in range(epochs):\n","        running_loss = 0.0\n","        for i, data in enumerate(train_loader, 0):\n","            inputs, labels = data[0].to(device), data[1].to(device)\n","\n","            optimizer.zero_grad()\n","\n","            outputs = net(inputs)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss += loss.item()\n","        \n","        train_losses.append(running_loss / len(train_loader))\n","        val_loss = validate(net, val_loader, criterion)\n","        val_losses.append(val_loss)\n","        print(f'Epoch {epoch + 1}, Train Loss: {train_losses[-1]}, Validation Loss: {val_losses[-1]}')\n","\n","    return train_losses, val_losses"]},{"cell_type":"markdown","metadata":{},"source":["Metrics Calculation"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["# Function to calculate accuracy, F1 score, and recall\n","def calculate_metrics(net, loader):\n","    net.eval()\n","    correct = 0\n","    total = 0\n","    all_labels = []\n","    all_predictions = []\n","    \n","    with torch.no_grad():\n","        for data in loader:\n","            images, labels = data[0].to(device), data[1].to(device)\n","            outputs = net(images)\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","            all_labels.extend(labels.cpu().numpy())\n","            all_predictions.extend(predicted.cpu().numpy())\n","\n","    accuracy = 100 * correct / total\n","    f1 = f1_score(all_labels, all_predictions, average='weighted')\n","    recall = recall_score(all_labels, all_predictions, average='weighted')\n","    return accuracy, f1, recall"]},{"cell_type":"markdown","metadata":{},"source":["Test Error Calculation"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["# Additional function to calculate test error\n","def calculate_test_error(net, test_loader):\n","    net.eval()\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for data in test_loader:\n","            images, labels = data[0].to(device), data[1].to(device)\n","            outputs = net(images)\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","    test_accuracy = 100 * correct / total\n","    return 100 - test_accuracy  # Test error"]},{"cell_type":"markdown","metadata":{},"source":["Model Architecture"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["class ChannelAttention(nn.Module):\n","    def __init__(self, in_planes, ratio=16):\n","        super(ChannelAttention, self).__init__()\n","        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n","        self.max_pool = nn.AdaptiveMaxPool2d(1)\n","        self.fc1   = nn.Conv2d(in_planes, in_planes // ratio, 1, bias=False)\n","        self.relu1 = nn.ReLU()\n","        self.fc2   = nn.Conv2d(in_planes // ratio, in_planes, 1, bias=False)\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x))))\n","        max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x))))\n","        out = avg_out + max_out\n","        return self.sigmoid(out)\n","\n","def initialize_model(device):\n","    class DecentFitCNN(nn.Module):\n","        def __init__(self):\n","            super(DecentFitCNN, self).__init__()\n","            # Convolutional layers\n","            self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n","            self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n","            self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n","\n","            # Channel Attention layers\n","            self.ca1 = ChannelAttention(32)\n","            self.ca2 = ChannelAttention(64)\n","            self.ca3 = ChannelAttention(128)\n","\n","            # Batch normalization layers\n","            self.bn1 = nn.BatchNorm2d(32)\n","            self.bn2 = nn.BatchNorm2d(64)\n","            self.bn3 = nn.BatchNorm2d(128)\n","\n","            # Max pooling\n","            self.pool = nn.MaxPool2d(2, 2)\n","\n","            # Dropout\n","            self.dropout = nn.Dropout(0.2)\n","\n","            # Fully connected layers\n","            self.fc1 = nn.Linear(128 * 4 * 4, 512)\n","            self.fc2 = nn.Linear(512, 256)\n","            self.fc3 = nn.Linear(256, 10)  # 10 classes in CIFAR-10\n","\n","        def forward(self, x):\n","            # Convolutional layers with ReLU, batch normalization, and channel attention\n","            x = self.ca1(self.bn1(F.relu(self.conv1(x))))\n","            x = self.pool(x)\n","            x = self.ca2(self.bn2(F.relu(self.conv2(x))))\n","            x = self.pool(x)\n","            x = self.ca3(self.bn3(F.relu(self.conv3(x))))\n","            x = self.pool(x)\n","\n","            # Flatten the tensor for the fully connected layers\n","            x = x.view(-1, 128 * 4 * 4)\n","\n","            # Fully connected layers with dropout\n","            x = F.relu(self.fc1(x))\n","            x = self.dropout(x)\n","            x = F.relu(self.fc2(x))\n","            x = self.fc3(x)\n","            return x\n","\n","    net = DecentFitCNN().to(device)\n","    return net"]},{"cell_type":"markdown","metadata":{},"source":["Loss Function"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["def initialize_loss_function():\n","    return nn.CrossEntropyLoss()"]},{"cell_type":"markdown","metadata":{},"source":["Optimizer"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'SWATS_optimizer'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[1;32mc:\\PennState OneDrive\\OneDrive - The Pennsylvania State University\\CODE\\DF_CNN_AL\\DF_CNN_AL_SWATS_200.ipynb Cell 22\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/PennState%20OneDrive/OneDrive%20-%20The%20Pennsylvania%20State%20University/CODE/DF_CNN_AL/DF_CNN_AL_SWATS_200.ipynb#X30sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mSWATS_optimizer\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moptim\u001b[39;00m \u001b[39mimport\u001b[39;00m SWATS\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/PennState%20OneDrive/OneDrive%20-%20The%20Pennsylvania%20State%20University/CODE/DF_CNN_AL/DF_CNN_AL_SWATS_200.ipynb#X30sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minitialize_optimizer\u001b[39m(model_params):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/PennState%20OneDrive/OneDrive%20-%20The%20Pennsylvania%20State%20University/CODE/DF_CNN_AL/DF_CNN_AL_SWATS_200.ipynb#X30sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m SWATS(model_params)\n","\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'SWATS_optimizer'"]}],"source":["from SWATS_optimizer.optim import SWATS\n","\n","def initialize_optimizer(model_params):\n","    return SWATS(model_params)"]},{"cell_type":"markdown","metadata":{},"source":["Training the model for different seeds"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Run the training for 5 different seeds\n","epochs = 200\n","metrics = {\n","    'seed': [],\n","    'train_losses': [],\n","    'val_losses': [],\n","    'val_accuracy': [],\n","    'val_f1': [],\n","    'val_recall': [],\n","    'test_accuracy': [],\n","    'test_f1': [],\n","    'test_recall': []\n","}\n","\n","all_test_errors = []\n","\n","for run in range(5):\n","    print(f\"Run: {run + 1}\")\n","    seed = set_seed()\n","    print(f\"Seed: {seed}\")\n","\n","    # Reinitialize the model, loss function, and optimizer\n","    net = initialize_model(device=device)\n","    criterion = initialize_loss_function()\n","    optimizer = initialize_optimizer(model_params=net.parameters())\n","\n","    # Train the network\n","    train_losses, val_losses = train_model(net, train_loader, val_loader, criterion, optimizer, epochs)\n","\n","    # Calculate metrics on validation and test sets\n","    val_accuracy, val_f1, val_recall = calculate_metrics(net, val_loader)\n","    test_accuracy, test_f1, test_recall = calculate_metrics(net, test_loader)\n","    print(f\"Run {run+1} - Validation Accuracy: {val_accuracy}, F1: {val_f1}, Recall: {val_recall}\")\n","    print(f\"Run {run+1} - Test Accuracy: {test_accuracy}, F1: {test_f1}, Recall: {test_recall}\")\n","\n","    # Plot training and validation loss\n","    plt.figure(figsize=(10, 5))\n","    plt.plot(train_losses, label='Training Loss')\n","    plt.plot(val_losses, label='Validation Loss')\n","    plt.title(f\"Training and Validation Loss for Run {run+1}\")\n","    plt.xlabel('Epochs')\n","    plt.ylabel('Loss')\n","    plt.legend()\n","    plt.show()\n","    \n","    # Calculate test error\n","    test_errors = [calculate_test_error(net, test_loader) for _ in train_losses]  # Assuming constant test error over epochs\n","    all_test_errors.append(test_errors)\n","    \n","    # Plot test error\n","    plt.figure(figsize=(10, 5))\n","    plt.plot(test_errors, label='Test Error')\n","    plt.title(f\"Test Error for Run {run+1}\")\n","    plt.xlabel('Epochs')\n","    plt.ylabel('Error (%)')\n","    plt.legend()\n","    plt.show()\n","    \n","    # Store metrics\n","    metrics['seed'].append(seed)\n","    metrics['train_losses'].append(train_losses)\n","    metrics['val_losses'].append(val_losses)\n","    metrics['val_accuracy'].append(val_accuracy)\n","    metrics['val_f1'].append(val_f1)\n","    metrics['val_recall'].append(val_recall)\n","    metrics['test_accuracy'].append(test_accuracy)\n","    metrics['test_f1'].append(test_f1)\n","    metrics['test_recall'].append(test_recall)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Calculate mean and standard deviation across runs\n","mean_metrics = {metric: np.mean(metrics[metric]) for metric in metrics if metric not in ['seed', 'train_losses', 'val_losses']}\n","std_metrics = {metric: np.std(metrics[metric]) for metric in metrics if metric not in ['seed', 'train_losses', 'val_losses']}\n","\n","# Print mean metrics and standard deviation\n","print(\"Mean Metrics:\", mean_metrics)\n","print(\"Standard Deviation of Metrics:\", std_metrics)\n","\n","# Calculate mean test error across all runs\n","mean_test_errors = np.mean(all_test_errors, axis=0)\n","\n","# Plot mean test error\n","plt.figure(figsize=(10, 5))\n","plt.plot(mean_test_errors, label='Mean Test Error')\n","plt.title(\"Mean Test Error Across All Runs\")\n","plt.xlabel('Epochs')\n","plt.ylabel('Error (%)')\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","import torch\n","\n","# Assume all_test_errors is a list of test errors for each run\n","test_error = all_test_errors[0]  # Selecting the test errors of the first run\n","\n","# Define the directory path where the file will be saved\n","directory_path = '../test_errors/'\n","\n","# Create the directory if it doesn't exist\n","os.makedirs(directory_path, exist_ok=True)\n","\n","# Save the test errors to a file in the specified directory\n","file_name = 'DF_CNN_AL_SWATS_200.pt'  # Name the file\n","torch.save(test_error, os.path.join(directory_path, file_name))\n","\n","# Providing the path where the file is saved\n","path_to_saved_file = os.path.join(directory_path, file_name)\n","print(\"Path to saved file:\", path_to_saved_file)\n"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30588,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":4}
