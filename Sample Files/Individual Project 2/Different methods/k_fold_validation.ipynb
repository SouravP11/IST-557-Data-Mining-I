{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Import Statements"
      ],
      "metadata": {
        "id": "jtGLhMX5Dqb-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import random_split\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "3cymbevFDtaB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set the Seed for reproducibility"
      ],
      "metadata": {
        "id": "Mr3yKeOHDvjx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the random seed for reproducibility\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)"
      ],
      "metadata": {
        "id": "FrIxfehUDyvI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define number of folds"
      ],
      "metadata": {
        "id": "RxjqoAdoD1VY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the number of folds (k)\n",
        "k = 5  # You can change this value as needed"
      ],
      "metadata": {
        "id": "cPCIcmaeD3-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CNN"
      ],
      "metadata": {
        "id": "YCV9wC-fD5zX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define your CNN model\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(512, 10)  # 10 classes for CIFAR-10\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "rnul8ygSD714"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss and Optimizer"
      ],
      "metadata": {
        "id": "EiV8wbzgD_5y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define optimizer and loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "PCbortoaECpB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Prep"
      ],
      "metadata": {
        "id": "3IxlgUK5EFrY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define data augmentation and preprocessing transformations\n",
        "batch_size = 20\n",
        "augmentation_transform = transforms.Compose([\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.RandomResizedCrop(32, scale=(0.8, 1.0)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Load the CIFAR-10 dataset\n",
        "augmented_train_data = torchvision.datasets.CIFAR10('data', train=True, download=True, transform=augmentation_transform)\n",
        "\n",
        "# Split the dataset into k folds\n",
        "fold_size = len(augmented_train_data) // k\n",
        "fold_datasets = [torch.utils.data.Subset(augmented_train_data, range(i * fold_size, (i + 1) * fold_size)) for i in range(k)]\n",
        "\n",
        "# 10 classes in total\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "metadata": {
        "id": "og45j__PEHrl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test set"
      ],
      "metadata": {
        "id": "l6-TOF-GFM1o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load test data (note that the data has been transformed already)\n",
        "test_images = torch.load('/kaggle/input/testtest/test_image.pt')"
      ],
      "metadata": {
        "id": "L2XS7W2WFOgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Loop"
      ],
      "metadata": {
        "id": "zLhvOPZUEQ9R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8k4IH8XLDlnz"
      },
      "outputs": [],
      "source": [
        "# Lists to store performance metrics\n",
        "all_fold_train_losses = []\n",
        "all_fold_val_losses = []\n",
        "\n",
        "for fold_idx, fold_data in enumerate(fold_datasets):\n",
        "    print(f\"Training fold {fold_idx + 1}/{k}\")\n",
        "\n",
        "    # Create data loaders for training and validation\n",
        "    validation_set = fold_data\n",
        "    training_sets = [f for i, f in enumerate(fold_datasets) if i != fold_idx]\n",
        "    training_set = torch.utils.data.ConcatDataset(training_sets)\n",
        "\n",
        "    train_loader = DataLoader(training_set, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "    valid_loader = DataLoader(validation_set, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    # Initialize your model\n",
        "    model = CNN()\n",
        "    model.to(device)\n",
        "\n",
        "    # Initialize optimizer and loss function\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "    # Training loop\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    total_epoch = 20  # You can change the number of epochs as needed\n",
        "\n",
        "    for epoch in range(total_epoch):\n",
        "        train_loss = 0.0\n",
        "        valid_loss = 0.0\n",
        "\n",
        "        # Train the model\n",
        "        model.train()\n",
        "        for data, target in train_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item() * data.size(0)\n",
        "\n",
        "        # Validate the model\n",
        "        model.eval()\n",
        "        for valid_data, valid_target in valid_loader:\n",
        "            valid_data, valid_target = valid_data.to(device), valid_target.to(device)\n",
        "            valid_output = model(valid_data)\n",
        "            loss = criterion(valid_output, valid_target)\n",
        "            valid_loss += loss.item() * valid_data.size(0)\n",
        "\n",
        "        avg_val_loss = valid_loss / len(validation_set)\n",
        "        avg_train_loss = train_loss / len(training_set)\n",
        "\n",
        "        val_losses.append(avg_val_loss)\n",
        "        train_losses.append(avg_train_loss)\n",
        "\n",
        "        print(f'Epoch: {epoch + 1}/{total_epoch} \\tTraining Loss: {avg_train_loss:.6f} \\tValidation Loss: {avg_val_loss:.6f}')\n",
        "\n",
        "    print(f'Finished Training for fold {fold_idx + 1}/{k}')\n",
        "    all_fold_train_losses.append(train_losses)\n",
        "    all_fold_val_losses.append(val_losses)\n",
        "\n",
        "# Calculate and print the average performance metrics over all folds\n",
        "avg_train_losses = np.mean(all_fold_train_losses, axis=0)\n",
        "avg_val_losses = np.mean(all_fold_val_losses, axis=0)\n",
        "\n",
        "# Plot the training and validation loss curves\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(range(1, total_epoch + 1), avg_train_losses, label='Average Train Loss', marker='o')\n",
        "plt.plot(range(1, total_epoch + 1), avg_val_losses, label='Average Validation Loss', marker='o')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title(f'Average Training and Validation Loss Curves ({k}-fold cross-validation)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# since now we're testing (not training), we set no_grad to NOT calculate the gradients\n",
        "with torch.no_grad():\n",
        "    test_images = test_images.to(device)\n",
        "    # calculate outputs by running images through the network\n",
        "    outputs = model(test_images)\n",
        "    # the class with the highest probability is what we choose as prediction\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    predicted = np.array([classes[i] for i in predicted])\n",
        "\n",
        "submission = pd.DataFrame()\n",
        "submission['label'] = predicted\n",
        "submission.to_csv(\"kFold\", index=True, index_label='id')\n",
        "submission"
      ],
      "metadata": {
        "id": "Q7YKagF7FUKJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}